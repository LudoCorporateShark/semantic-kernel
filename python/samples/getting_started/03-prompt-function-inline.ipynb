{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c93ac5b",
   "metadata": {},
   "source": [
    "# Running Prompt Functions Inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85226c",
   "metadata": {},
   "source": [
    "Import Semantic Kernel SDK from pypi.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a94d2b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: semantic-kernel==1.5.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.8 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (3.10.3)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (0.7.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.3 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (3.1.4)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.25 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (2.0.1)\n",
      "Requirement already satisfied: openai>=1.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (1.40.5)\n",
      "Requirement already satisfied: openapi_core<0.20,>=0.18 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (0.19.2)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.24.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (1.26.0)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.2.2 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (2.2.2)\n",
      "Requirement already satisfied: prance<24.0.0.0,>=23.6.21.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (23.6.21.0)\n",
      "Requirement already satisfied: pybars4<0.10.0,>=0.9.13 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (0.9.13)\n",
      "Requirement already satisfied: pydantic<3,>=2 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (2.8.2)\n",
      "Requirement already satisfied: pydantic-settings<3,>=2 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from semantic-kernel==1.5.0) (2.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==1.5.0) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==1.5.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==1.5.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==1.5.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==1.5.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from aiohttp<4.0,>=3.8->semantic-kernel==1.5.0) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from jinja2<4.0.0,>=3.1.3->semantic-kernel==1.5.0) (2.1.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openai>=1.0->semantic-kernel==1.5.0) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openai>=1.0->semantic-kernel==1.5.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openai>=1.0->semantic-kernel==1.5.0) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openai>=1.0->semantic-kernel==1.5.0) (0.5.0)\n",
      "Requirement already satisfied: sniffio in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openai>=1.0->semantic-kernel==1.5.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openai>=1.0->semantic-kernel==1.5.0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openai>=1.0->semantic-kernel==1.5.0) (4.12.2)\n",
      "Requirement already satisfied: isodate in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (0.6.1)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (4.23.0)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (0.3.3)\n",
      "Requirement already satisfied: more-itertools in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (10.4.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (0.6.2)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (0.7.1)\n",
      "Requirement already satisfied: parse in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (1.20.2)\n",
      "Requirement already satisfied: werkzeug in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (3.0.3)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.24.0->semantic-kernel==1.5.0) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.24.0->semantic-kernel==1.5.0) (8.0.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->semantic-kernel==1.5.0) (0.47b0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from pandas<3.0.0,>=2.2.2->semantic-kernel==1.5.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from pandas<3.0.0,>=2.2.2->semantic-kernel==1.5.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from pandas<3.0.0,>=2.2.2->semantic-kernel==1.5.0) (2024.1)\n",
      "Requirement already satisfied: chardet>=3.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==1.5.0) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==1.5.0) (0.18.6)\n",
      "Requirement already satisfied: requests>=2.25 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==1.5.0) (2.32.3)\n",
      "Requirement already satisfied: six~=1.15 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==1.5.0) (1.16.0)\n",
      "Requirement already satisfied: packaging>=21.3 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel==1.5.0) (24.1)\n",
      "Requirement already satisfied: PyMeta3>=0.5.1 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from pybars4<0.10.0,>=0.9.13->semantic-kernel==1.5.0) (0.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from pydantic<3,>=2->semantic-kernel==1.5.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from pydantic<3,>=2->semantic-kernel==1.5.0) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from pydantic-settings<3,>=2->semantic-kernel==1.5.0) (1.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.0->semantic-kernel==1.5.0) (3.7)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.24.0->semantic-kernel==1.5.0) (1.16.0)\n",
      "Requirement already satisfied: certifi in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==1.5.0) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==1.5.0) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->semantic-kernel==1.5.0) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api<2.0.0,>=1.24.0->semantic-kernel==1.5.0) (3.20.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (0.20.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (6.0.2)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (0.4.3)\n",
      "Requirement already satisfied: rfc3339-validator in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (0.1.4)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.20,>=0.18->semantic-kernel==1.5.0) (1.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==1.5.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==1.5.0) (2.2.2)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from ruamel.yaml>=0.17.10->prance<24.0.0.0,>=23.6.21.0->semantic-kernel==1.5.0) (0.2.8)\n",
      "Requirement already satisfied: colorama in d:\\codebase docs\\new folder\\semantic-kernel\\.venv\\lib\\site-packages (from tqdm>4->openai>=1.0->semantic-kernel==1.5.0) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Note: if using a Poetry virtual environment, do not run this cell\n",
    "%pip install semantic-kernel==1.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704e54b",
   "metadata": {},
   "source": [
    "Initial configuration for the notebook to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebb95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure paths are correct for the imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "\n",
    "\n",
    "sys.path.append(grandparent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb77b5",
   "metadata": {},
   "source": [
    "### Configuring the Kernel\n",
    "\n",
    "Let's get started with the necessary configuration to run Semantic Kernel. For Notebooks, we require a `.env` file with the proper settings for the model you use. Create a new file named `.env` and place it in this directory. Copy the contents of the `.env.example` file from this directory and paste it into the `.env` file that you just created.\n",
    "\n",
    "**NOTE: Please make sure to include `GLOBAL_LLM_SERVICE` set to either OpenAI, AzureOpenAI, or HuggingFace in your .env file. If this setting is not included, the Service will default to AzureOpenAI.**\n",
    "\n",
    "#### Option 1: using OpenAI\n",
    "\n",
    "Add your [OpenAI Key](https://openai.com/product/) key to your `.env` file (org Id only if you have multiple orgs):\n",
    "\n",
    "```\n",
    "GLOBAL_LLM_SERVICE=\"OpenAI\"\n",
    "OPENAI_API_KEY=\"sk-...\"\n",
    "OPENAI_ORG_ID=\"\"\n",
    "OPENAI_CHAT_MODEL_ID=\"\"\n",
    "OPENAI_TEXT_MODEL_ID=\"\"\n",
    "OPENAI_EMBEDDING_MODEL_ID=\"\"\n",
    "```\n",
    "The names should match the names used in the `.env` file, as shown above.\n",
    "\n",
    "#### Option 2: using Azure OpenAI\n",
    "\n",
    "Add your [Azure Open AI Service key](https://learn.microsoft.com/azure/cognitive-services/openai/quickstart?pivots=programming-language-studio) settings to the `.env` file in the same folder:\n",
    "\n",
    "```\n",
    "GLOBAL_LLM_SERVICE=\"AzureOpenAI\"\n",
    "AZURE_OPENAI_API_KEY=\"...\"\n",
    "AZURE_OPENAI_ENDPOINT=\"https://...\"\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=\"...\"\n",
    "AZURE_OPENAI_TEXT_DEPLOYMENT_NAME=\"...\"\n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME=\"...\"\n",
    "AZURE_OPENAI_API_VERSION=\"...\"\n",
    "```\n",
    "The names should match the names used in the `.env` file, as shown above.\n",
    "\n",
    "For more advanced configuration, please follow the steps outlined in the [setup guide](./CONFIGURING_THE_KERNEL.md)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40201641",
   "metadata": {},
   "source": [
    "The [previous notebook](./02-running-prompts-from-file.ipynb)\n",
    "showed how to define a semantic function using a prompt template stored on a file.\n",
    "\n",
    "In this notebook, we'll show how to use the Semantic Kernel to define functions inline with your python code. This can be useful in a few scenarios:\n",
    "\n",
    "- Dynamically generating the prompt using complex rules at runtime\n",
    "- Writing prompts by editing Python code instead of TXT files.\n",
    "- Easily creating demos, like this document\n",
    "\n",
    "Prompt templates are defined using the SK template language, which allows to reference variables and functions. Read [this doc](https://aka.ms/sk/howto/configurefunction) to learn more about the design decisions for prompt templating.\n",
    "\n",
    "For now we'll use only the `{{$input}}` variable, and see more complex templates later.\n",
    "\n",
    "Almost all semantic function prompts have a reference to `{{$input}}`, which is the default way\n",
    "a user can import content from the context variables.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d90b0c13",
   "metadata": {},
   "source": [
    "Prepare a semantic kernel instance first, loading also the AI service settings defined in the [Setup notebook](00-getting-started.ipynb):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377f42e",
   "metadata": {},
   "source": [
    "Let's define our kernel for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462b281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.kernel import Kernel\n",
    "\n",
    "kernel = Kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "734d121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 44\n",
      "Python-dotenv could not parse statement starting at line 68\n",
      "Python-dotenv could not parse statement starting at line 69\n",
      "Python-dotenv could not parse statement starting at line 71\n",
      "Python-dotenv could not parse statement starting at line 72\n",
      "Python-dotenv could not parse statement starting at line 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using service type: Service.AzureOpenAI\n"
     ]
    }
   ],
   "source": [
    "from services import Service\n",
    "\n",
    "from samples.service_settings import ServiceSettings\n",
    "\n",
    "service_settings = ServiceSettings.create()\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = (\n",
    "    Service.AzureOpenAI\n",
    "    if service_settings.global_llm_service is None\n",
    "    else Service(service_settings.global_llm_service.lower())\n",
    ")\n",
    "print(f\"Using service type: {selectedService}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06740170",
   "metadata": {},
   "source": [
    "We now configure our Chat Completion service on the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3712b7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 44\n",
      "Python-dotenv could not parse statement starting at line 68\n",
      "Python-dotenv could not parse statement starting at line 69\n",
      "Python-dotenv could not parse statement starting at line 71\n",
      "Python-dotenv could not parse statement starting at line 72\n",
      "Python-dotenv could not parse statement starting at line 74\n"
     ]
    }
   ],
   "source": [
    "# Remove all services so that this cell can be re-run without restarting the kernel\n",
    "kernel.remove_all_services()\n",
    "\n",
    "service_id = None\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "589733c5",
   "metadata": {},
   "source": [
    "Let's use a prompt to create a semantic function used to summarize content, allowing for some creativity and a sufficient number of tokens.\n",
    "\n",
    "The function will take in input the text to summarize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae29c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig\n",
    "\n",
    "prompt = \"\"\"{{$input}}\n",
    "Summarize the content above.\n",
    "\"\"\"\n",
    "\n",
    "if selectedService == Service.OpenAI:\n",
    "    execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt-3.5-turbo\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    execution_settings = AzureChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt4\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(  # semantic plugin configuration\n",
    "    template=prompt,                    # plugin prompt\n",
    "    name=\"summarize\",                   # plugin name\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "summarize = kernel.add_function(                # loading semantic plugin\n",
    "    function_name=\"summarizeFunc\",\n",
    "    plugin_name=\"summarizePlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f26b90c4",
   "metadata": {},
   "source": [
    "Set up some content to summarize, here's an extract about Demo, an ancient Greek poet, taken from Wikipedia (https://en.wikipedia.org/wiki/Demo_(ancient_Greek_poet)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "314557fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Demo (ancient Greek poet)\n",
    "From Wikipedia, the free encyclopedia\n",
    "Demo or Damo (Greek: Δεμώ, Δαμώ; fl. c. AD 200) was a Greek woman of the Roman period, known for a single epigram, engraved upon the Colossus of Memnon, which bears her name. She speaks of herself therein as a lyric poetess dedicated to the Muses, but nothing is known of her life.[1]\n",
    "Identity\n",
    "Demo was evidently Greek, as her name, a traditional epithet of Demeter, signifies. The name was relatively common in the Hellenistic world, in Egypt and elsewhere, and she cannot be further identified. The date of her visit to the Colossus of Memnon cannot be established with certainty, but internal evidence on the left leg suggests her poem was inscribed there at some point in or after AD 196.[2]\n",
    "Epigram\n",
    "There are a number of graffiti inscriptions on the Colossus of Memnon. Following three epigrams by Julia Balbilla, a fourth epigram, in elegiac couplets, entitled and presumably authored by \"Demo\" or \"Damo\" (the Greek inscription is difficult to read), is a dedication to the Muses.[2] The poem is traditionally published with the works of Balbilla, though the internal evidence suggests a different author.[1]\n",
    "In the poem, Demo explains that Memnon has shown her special respect. In return, Demo offers the gift for poetry, as a gift to the hero. At the end of this epigram, she addresses Memnon, highlighting his divine status by recalling his strength and holiness.[2]\n",
    "Demo, like Julia Balbilla, writes in the artificial and poetic Aeolic dialect. The language indicates she was knowledgeable in Homeric poetry—'bearing a pleasant gift', for example, alludes to the use of that phrase throughout the Iliad and Odyssey.[a][2]\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf0f2330",
   "metadata": {},
   "source": [
    "...and run the summary function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b0e3b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo, also known as Damo, was a Greek poetess from around 200 AD, during the Roman era. She is known for a single epigram found on the Colossus of Memnon. The epigram identifies her as a lyric poet dedicated to the Muses, but no other details about her life are known. Her name, common in the Hellenistic world, implies Greek origin and a possible connection to Demeter. The date of her inscription on the Colossus is uncertain but is thought to be after 196 AD.\n",
      "\n",
      "Demo's epigram is one of several inscriptions on the Colossus of Memnon, and it is distinct from other epigrams, such as those by Julia Balbilla. In her poem, Demo expresses that Memnon has honored her, and in gratitude, she dedicates her poetic talent to the hero, acknowledging his divine nature. She writes in the Aeolic dialect, a style that suggests familiarity with Homeric poetry.\n"
     ]
    }
   ],
   "source": [
    "summary = await kernel.invoke(summarize, input=input_text)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c2c1262",
   "metadata": {},
   "source": [
    "# Using ChatCompletion for Semantic Plugins\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29b59b28",
   "metadata": {},
   "source": [
    "You can also use chat completion models (like `gpt-35-turbo` and `gpt4`) for creating plugins. Normally you would have to tweak the API to accommodate for a system and user role, but SK abstracts that away for you by using `kernel.add_service` and `AzureChatCompletion` or `OpenAIChatCompletion`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4777f447",
   "metadata": {},
   "source": [
    "Here's one more example of how to write an inline Semantic Function that gives a TLDR for a piece of text using a ChatCompletion model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5886aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.remove_all_services()\n",
    "\n",
    "service_id = None\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea8128c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Robots: Don't harm, obey, survive.\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings, OpenAIChatPromptExecutionSettings\n",
    "\n",
    "prompt = \"\"\"\n",
    "{{$input}}\n",
    "\n",
    "Give me the TLDR in 5 words or less.\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\"\n",
    "    1) A robot may not injure a human being or, through inaction,\n",
    "    allow a human being to come to harm.\n",
    "\n",
    "    2) A robot must obey orders given it by human beings except where\n",
    "    such orders would conflict with the First Law.\n",
    "\n",
    "    3) A robot must protect its own existence as long as such protection\n",
    "    does not conflict with the First or Second Law.\n",
    "\"\"\"\n",
    "\n",
    "if selectedService == Service.OpenAI:\n",
    "    execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt-3.5-turbo\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    execution_settings = AzureChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt4\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,                    # prompt template\n",
    "    name=\"tldr\",                        # prompt template name\n",
    "    template_format=\"semantic-kernel\",  # 'semantic-kernel', 'jinja2' or 'handlebars'\n",
    "    input_variables=[                   # prompt variables\n",
    "        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    # allow_dangerously_set_content = True # unencoded input, safe content\n",
    "    execution_settings=execution_settings,  # prompt settings\n",
    ")\n",
    "\n",
    "tldr_function = kernel.add_function(  # Can also add a list of functions with 'add_functions()'\n",
    "    function_name=\"tldrFunction\",  # the function name to plug-in the prompt\n",
    "    plugin_name=\"tldrPlugin\",  # the function to plug-in the prompt\n",
    "    prompt_template_config=prompt_template_config,\n",
    "    return_plugin=False,  # switch between plugin (T) and function (F)\n",
    "    description=\"TLDR too long; didn't read\",\n",
    "    fully_qualified_name=\"TLDR_Function\",\n",
    ")\n",
    "\n",
    "# summary = await kernel.invoke(\n",
    "#     tldr_function,      # function(s) to execute, added to the kernel; semantic_configuration\n",
    "#     input=text          # prompt: system message + user input\n",
    "# )\n",
    "\n",
    "summary = await kernel.invoke(tldr_function, input=text)\n",
    "print(f\"Output: {summary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
